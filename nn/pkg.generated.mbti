// Generated using `moon info`, DON'T EDIT IT
package "2459822546/ml/nn"

import(
  "2459822546/ml/math"
)

// Values
fn act_relu() -> Activation

fn act_sigmoid() -> Activation

fn act_softmax() -> Activation

fn act_tanh() -> Activation

fn activation_forward(@math.Matrix, Activation) -> @math.Matrix

fn activation_forward_tensor(@math.Tensor, Activation) -> @math.Tensor

fn binary_cross_entropy(Array[Double], Array[Double], eps? : Double) -> Double

fn clip_grad_norm(Array[OptimParam], Double, eps? : Double) -> Double

fn clip_grad_value(Array[OptimParam], Double) -> Unit

fn cosine_decay(Double, Double, Int, Int) -> Double

fn cross_entropy(Array[Int], @math.Matrix) -> Double

fn early_stopping_max(Int, min_delta? : Double) -> EarlyStopping

fn early_stopping_min(Int, min_delta? : Double) -> EarlyStopping

fn focal_loss_binary(Array[Double], Array[Double], gamma? : Double, alpha? : Double, eps? : Double) -> Double

fn hinge_loss(Array[Double], Array[Double]) -> Double

fn huber_loss(Array[Double], Array[Double], delta? : Double) -> Double

fn kl_divergence(Array[Double], Array[Double]) -> Double

fn l1_penalty(Array[Double], Double) -> Double

fn l2_penalty(Array[Double], Double) -> Double

fn label_smoothing_cross_entropy(Array[Int], @math.Matrix, smoothing? : Double) -> Double

fn linear_rand_init(Int, Int, scale? : Double, seed? : Int) -> Linear

fn linear_warmup(Double, Int, Int) -> Double

fn mae_loss(Array[Double], Array[Double]) -> Double

fn mlp_train_loop(MLP, @data.DataLoader, Int, (MLP, Array[Array[Double]], Array[Double], Int, Int) -> MLP) -> MLP

fn mse_loss(Array[Double], Array[Double]) -> Double

fn multilabel_binary_cross_entropy(Array[Array[Double]], Array[Array[Double]], eps? : Double) -> Double

fn optimizer_adagrad(Adagrad) -> Optimizer

fn optimizer_adam(Adam) -> Optimizer

fn optimizer_adamw(AdamW) -> Optimizer

fn optimizer_get_lr(Optimizer) -> Double

fn optimizer_rmsprop(RMSProp) -> Optimizer

fn optimizer_set_lr(Optimizer, Double) -> Optimizer

fn optimizer_sgd(SGD) -> Optimizer

fn optimizer_step(Optimizer, OptimParam) -> Optimizer

fn smooth_l1_loss(Array[Double], Array[Double], beta? : Double) -> Double

fn step_decay(Double, Double, Int, Int) -> Double

fn warmup_cosine(Double, Double, Int, Int, Int) -> Double

// Errors

// Types and methods
pub enum Activation {
  Relu
  Sigmoid
  Tanh
  Softmax
}

pub struct Adagrad {
  mut lr : Double
  eps : Double
  h : Array[Double]
}
fn Adagrad::get_lr(Self) -> Double
fn Adagrad::new(Double, Int, eps? : Double) -> Self
fn Adagrad::set_lr(Self, Double) -> Unit
fn Adagrad::step(Self, OptimParam) -> Unit

pub struct Adam {
  mut lr : Double
  beta1 : Double
  beta2 : Double
  eps : Double
  m : Array[Double]
  v : Array[Double]
  mut t : Int
}
fn Adam::get_lr(Self) -> Double
fn Adam::new(Double, Int, beta1? : Double, beta2? : Double, eps? : Double) -> Self
fn Adam::set_lr(Self, Double) -> Unit
fn Adam::step(Self, OptimParam) -> Unit

pub struct AdamW {
  mut lr : Double
  beta1 : Double
  beta2 : Double
  eps : Double
  weight_decay : Double
  m : Array[Double]
  v : Array[Double]
  mut t : Int
}
fn AdamW::get_lr(Self) -> Double
fn AdamW::new(Double, Int, beta1? : Double, beta2? : Double, eps? : Double, weight_decay? : Double) -> Self
fn AdamW::set_lr(Self, Double) -> Unit
fn AdamW::step(Self, OptimParam) -> Unit

pub struct BatchNorm1d {
  gamma : Array[Double]
  beta : Array[Double]
  running_mean : Array[Double]
  running_var : Array[Double]
  momentum : Double
  eps : Double
}
fn BatchNorm1d::forward(Self, @math.Matrix, train? : Bool) -> @math.Matrix
fn BatchNorm1d::new(Int, momentum? : Double, eps? : Double) -> Self

pub struct ComputationGraph {
  nodes : Array[Node]
}
fn ComputationGraph::add(Self, Int, Int) -> Int
fn ComputationGraph::add_bias(Self, Int, Int) -> Int
fn ComputationGraph::add_input(Self, @math.Tensor, requires_grad? : Bool) -> Int
fn ComputationGraph::backward(Self, Int) -> Unit
fn ComputationGraph::backward_with_grad(Self, Int, @math.Tensor) -> Unit
fn ComputationGraph::grad_of(Self, Int) -> @math.Tensor
fn ComputationGraph::matmul(Self, Int, Int) -> Int
fn ComputationGraph::mul(Self, Int, Int) -> Int
fn ComputationGraph::new() -> Self
fn ComputationGraph::relu(Self, Int) -> Int
fn ComputationGraph::sub(Self, Int, Int) -> Int
fn ComputationGraph::sum(Self, Int) -> Int
fn ComputationGraph::value_of(Self, Int) -> @math.Tensor

pub struct Dropout {
  rate : Double
  mut seed : Int
}
fn Dropout::forward(Self, @math.Matrix, train? : Bool) -> @math.Matrix
fn Dropout::new(Double, seed? : Int) -> Self

pub struct EarlyStopping {
  mode : EarlyStoppingMode
  patience : Int
  min_delta : Double
  mut best : Double
  mut num_bad : Int
  mut stopped : Bool
}
fn EarlyStopping::get_best(Self) -> Double
fn EarlyStopping::new(EarlyStoppingMode, Int, min_delta? : Double) -> Self
fn EarlyStopping::reset(Self) -> Unit
fn EarlyStopping::should_stop(Self) -> Bool
fn EarlyStopping::update(Self, Double) -> Bool

pub enum EarlyStoppingMode {
  Min
  Max
}

pub struct Embedding {
  weight : Array[Array[Double]]
  num_embeddings : Int
  dim : Int
  seed : Int
}
fn Embedding::forward(Self, Array[Int]) -> @math.Matrix
fn Embedding::new(Int, Int, seed? : Int) -> Self

pub struct LayerNorm {
  gamma : Array[Double]
  beta : Array[Double]
  eps : Double
}
fn LayerNorm::forward(Self, @math.Matrix) -> @math.Matrix
fn LayerNorm::new(Int, eps? : Double) -> Self

pub struct Linear {
  mut w : Array[Array[Double]]
  mut b : Array[Double]
  in_features : Int
  out_features : Int
}
fn Linear::forward(Self, @math.Matrix) -> @math.Matrix
fn Linear::forward_tensor(Self, @math.Tensor) -> @math.Tensor
fn Linear::new(Int, Int) -> Self
fn Linear::set_params(Self, Array[Array[Double]], Array[Double]) -> Unit

pub struct MLP {
  layers : Array[Linear]
  activations : Array[Activation]
  output_activation : Activation
}
fn MLP::forward(Self, @math.Matrix) -> @math.Matrix
fn MLP::forward_tensor(Self, @math.Tensor) -> @math.Tensor
fn MLP::mlp_params(Self) -> Array[OptimParam]
fn MLP::mlp_predict(Self, @math.Matrix) -> Array[Int]
fn MLP::mlp_predict_proba(Self, @math.Matrix) -> @math.Matrix
fn MLP::mlp_predict_proba_tensor(Self, @math.Tensor) -> @math.Tensor
fn MLP::mlp_predict_tensor(Self, @math.Tensor) -> Array[Int]
fn MLP::mlp_set_params(Self, Array[OptimParam]) -> Unit
fn MLP::new(Int, Array[Int], Int, output_activation? : Activation, seed? : Int) -> Self

pub struct Node {
  value : @math.Tensor
  mut grad : @math.Tensor
  op : Op
  parents : Array[Int]
  requires_grad : Bool
}

pub enum Op {
  Input
  Add
  Sub
  Mul
  Relu
  MatMul
  AddBias
  Sum
}

pub struct OptimParam {
  data : Array[Double]
  grad : Array[Double]
}
fn OptimParam::new(Array[Double]) -> Self
fn OptimParam::with_grad(Self, Array[Double]) -> Self
fn OptimParam::zero_grad(Self) -> Unit

pub enum Optimizer {
  SGD(SGD)
  Adam(Adam)
  AdamW(AdamW)
  RMSProp(RMSProp)
  Adagrad(Adagrad)
}

pub struct RMSProp {
  mut lr : Double
  alpha : Double
  eps : Double
  v : Array[Double]
}
fn RMSProp::get_lr(Self) -> Double
fn RMSProp::new(Double, Int, alpha? : Double, eps? : Double) -> Self
fn RMSProp::set_lr(Self, Double) -> Unit
fn RMSProp::step(Self, OptimParam) -> Unit

pub struct SGD {
  mut lr : Double
  momentum : Double
  weight_decay : Double
  velocity : Array[Double]
}
fn SGD::get_lr(Self) -> Double
fn SGD::new(Double, Int, momentum? : Double, weight_decay? : Double) -> Self
fn SGD::set_lr(Self, Double) -> Unit
fn SGD::step(Self, OptimParam) -> Unit

// Type aliases

// Traits

