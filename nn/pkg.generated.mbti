// Generated using `moon info`, DON'T EDIT IT
package "2459822546/ml/nn"

import(
  "2459822546/ml/data"
  "2459822546/ml/math"
)

// Values
fn act_relu() -> Activation

fn act_sigmoid() -> Activation

fn act_softmax() -> Activation

fn act_tanh() -> Activation

fn activation_forward(@math.Matrix, Activation) -> @math.Matrix

fn activation_forward_tensor(@math.Tensor, Activation) -> @math.Tensor

fn binary_cross_entropy(Array[Double], Array[Double], eps? : Double) -> Double

fn clip_grad_norm(Array[OptimParam], Double, eps? : Double) -> Double

fn clip_grad_value(Array[OptimParam], Double) -> Unit

fn cosine_decay(Double, Double, Int, Int) -> Double

fn cross_entropy(Array[Int], @math.Matrix) -> Double

fn early_stopping_max(Int, min_delta? : Double) -> EarlyStopping

fn early_stopping_min(Int, min_delta? : Double) -> EarlyStopping

fn focal_loss_binary(Array[Double], Array[Double], gamma? : Double, alpha? : Double, eps? : Double) -> Double

fn hinge_loss(Array[Double], Array[Double]) -> Double

fn huber_loss(Array[Double], Array[Double], delta? : Double) -> Double

fn kl_divergence(Array[Double], Array[Double]) -> Double

fn l1_penalty(Array[Double], Double) -> Double

fn l2_penalty(Array[Double], Double) -> Double

fn label_smoothing_cross_entropy(Array[Int], @math.Matrix, smoothing? : Double) -> Double

fn linear_rand_init(Int, Int, scale? : Double, seed? : Int) -> Linear

fn linear_warmup(Double, Int, Int) -> Double

fn mae_loss(Array[Double], Array[Double]) -> Double

fn mlp_train_loop(MLP, @data.DataLoader, Int, (MLP, Array[Array[Double]], Array[Double], Int, Int) -> MLP) -> MLP

fn module_batchnorm1d(BatchNorm1d) -> Module

fn module_dropout(Dropout) -> Module

fn module_embedding(Embedding) -> Module

fn module_embedding_module(EmbeddingModule) -> Module

fn module_eval(Module) -> Module

fn module_forward(Module, AutoTensor, train? : Bool) -> (AutoTensor, ModuleContext)

fn module_layernorm(LayerNorm) -> Module

fn module_linear(LinearModule) -> Module

fn module_load_state_dict(Module, Map[String, @math.Tensor], strict? : Bool, prefix? : String) -> Unit

fn module_modulelist(ModuleList) -> Module

fn module_parameters(Module) -> Array[Parameter]

fn module_relu() -> Module

fn module_sequential(Sequential) -> Module

fn module_state_dict(Module, prefix? : String) -> Map[String, @math.Tensor]

fn module_sync_grads(Module, ModuleContext) -> Unit

fn module_train(Module) -> Module

fn mse_loss(Array[Double], Array[Double]) -> Double

fn multilabel_binary_cross_entropy(Array[Array[Double]], Array[Array[Double]], eps? : Double) -> Double

fn optimizer_adagrad(Adagrad) -> Optimizer

fn optimizer_adam(Adam) -> Optimizer

fn optimizer_adamw(AdamW) -> Optimizer

fn optimizer_get_lr(Optimizer) -> Double

fn optimizer_rmsprop(RMSProp) -> Optimizer

fn optimizer_set_lr(Optimizer, Double) -> Optimizer

fn optimizer_sgd(SGD) -> Optimizer

fn optimizer_step(Optimizer, OptimParam) -> Optimizer

fn smooth_l1_loss(Array[Double], Array[Double], beta? : Double) -> Double

fn step_decay(Double, Double, Int, Int) -> Double

fn warmup_cosine(Double, Double, Int, Int, Int) -> Double

// Errors

// Types and methods
pub enum Activation {
  Relu
  Sigmoid
  Tanh
  Softmax
}

pub struct Adagrad {
  mut lr : Double
  eps : Double
  h : Array[Double]
}
fn Adagrad::get_lr(Self) -> Double
fn Adagrad::new(Double, Int, eps? : Double) -> Self
fn Adagrad::set_lr(Self, Double) -> Unit
fn Adagrad::step(Self, OptimParam) -> Unit

pub struct AdagradParams {
  lr : Double
  eps : Double
  h : Array[Array[Double]]
}
fn AdagradParams::new(Double, Array[Parameter], eps? : Double) -> Self
fn AdagradParams::step(Self, Array[Parameter]) -> Unit
fn AdagradParams::step_seq(Self, Sequential) -> Unit

pub struct Adam {
  mut lr : Double
  beta1 : Double
  beta2 : Double
  eps : Double
  m : Array[Double]
  v : Array[Double]
  mut t : Int
}
fn Adam::get_lr(Self) -> Double
fn Adam::new(Double, Int, beta1? : Double, beta2? : Double, eps? : Double) -> Self
fn Adam::set_lr(Self, Double) -> Unit
fn Adam::step(Self, OptimParam) -> Unit

pub struct AdamParams {
  lr : Double
  beta1 : Double
  beta2 : Double
  eps : Double
  m : Array[Array[Double]]
  v : Array[Array[Double]]
  mut t : Int
}
fn AdamParams::new(Double, Array[Parameter], beta1? : Double, beta2? : Double, eps? : Double) -> Self
fn AdamParams::step(Self, Array[Parameter]) -> Unit
fn AdamParams::step_seq(Self, Sequential) -> Unit

pub struct AdamW {
  mut lr : Double
  beta1 : Double
  beta2 : Double
  eps : Double
  weight_decay : Double
  m : Array[Double]
  v : Array[Double]
  mut t : Int
}
fn AdamW::get_lr(Self) -> Double
fn AdamW::new(Double, Int, beta1? : Double, beta2? : Double, eps? : Double, weight_decay? : Double) -> Self
fn AdamW::set_lr(Self, Double) -> Unit
fn AdamW::step(Self, OptimParam) -> Unit

pub struct AdamWParams {
  lr : Double
  beta1 : Double
  beta2 : Double
  eps : Double
  weight_decay : Double
  m : Array[Array[Double]]
  v : Array[Array[Double]]
  mut t : Int
}
fn AdamWParams::new(Double, Array[Parameter], beta1? : Double, beta2? : Double, eps? : Double, weight_decay? : Double) -> Self
fn AdamWParams::step(Self, Array[Parameter]) -> Unit
fn AdamWParams::step_seq(Self, Sequential) -> Unit

pub struct AutoTensor {
  graph : ComputationGraph
  id : Int
}
fn AutoTensor::add(Self, Self) -> Self
fn AutoTensor::add_bias(Self, Self) -> Self
fn AutoTensor::backward(Self) -> Unit
fn AutoTensor::backward_with_grad(Self, @math.Tensor) -> Unit
fn AutoTensor::batchnorm1d_cached(Self, @math.Tensor, @math.Tensor, @math.Tensor, Self, Self, Double, train? : Bool) -> Self
fn AutoTensor::dropout(Self, Double, Int, train? : Bool) -> (Self, Int)
fn AutoTensor::embedding(Self, Self) -> Self
fn AutoTensor::from_other(Self, @math.Tensor, requires_grad? : Bool) -> Self
fn AutoTensor::grad(Self) -> @math.Tensor
fn AutoTensor::layernorm(Self, Self, Self, Double) -> Self
fn AutoTensor::matmul(Self, Self) -> Self
fn AutoTensor::mul(Self, Self) -> Self
fn AutoTensor::new(@math.Tensor, requires_grad? : Bool) -> Self
fn AutoTensor::relu(Self) -> Self
fn AutoTensor::sub(Self, Self) -> Self
fn AutoTensor::sum(Self) -> Self
fn AutoTensor::value(Self) -> @math.Tensor
impl Add for AutoTensor
impl Mul for AutoTensor
impl Sub for AutoTensor

pub struct BatchNorm1d {
  gamma : Array[Double]
  beta : Array[Double]
  running_mean : Array[Double]
  running_var : Array[Double]
  momentum : Double
  eps : Double
}
fn BatchNorm1d::forward(Self, @math.Matrix, train? : Bool) -> @math.Matrix
fn BatchNorm1d::forward_with_cache(Self, @math.Matrix, train? : Bool) -> (@math.Matrix, Array[Double], Array[Double])
fn BatchNorm1d::new(Int, momentum? : Double, eps? : Double) -> Self

pub struct BatchNorm1dContext {
  gamma : AutoTensor
  beta : AutoTensor
}

pub struct BatchNorm1dModule {
  gamma : Parameter
  beta : Parameter
  mut running_mean : Array[Double]
  mut running_var : Array[Double]
  momentum : Double
  eps : Double
}
fn BatchNorm1dModule::forward(Self, AutoTensor, train? : Bool) -> (AutoTensor, BatchNorm1dContext)
fn BatchNorm1dModule::from_layer(BatchNorm1d) -> Self
fn BatchNorm1dModule::new(Int, momentum? : Double, eps? : Double) -> Self
fn BatchNorm1dModule::parameters(Self) -> Array[Parameter]
fn BatchNorm1dModule::sync_grads(Self, BatchNorm1dContext) -> Unit

pub struct ComputationGraph {
  nodes : Array[Node]
}
fn ComputationGraph::add(Self, Int, Int) -> Int
fn ComputationGraph::add_bias(Self, Int, Int) -> Int
fn ComputationGraph::add_input(Self, @math.Tensor, requires_grad? : Bool) -> Int
fn ComputationGraph::backward(Self, Int) -> Unit
fn ComputationGraph::backward_with_grad(Self, Int, @math.Tensor) -> Unit
fn ComputationGraph::batchnorm1d_cached(Self, Int, Int, Int, @math.Tensor, @math.Tensor, @math.Tensor, Double, train? : Bool) -> Int
fn ComputationGraph::dropout(Self, Int, Double, Int) -> (Int, Int)
fn ComputationGraph::embedding(Self, Int, @math.Tensor) -> Int
fn ComputationGraph::grad_of(Self, Int) -> @math.Tensor
fn ComputationGraph::layernorm(Self, Int, Int, Int, Double) -> Int
fn ComputationGraph::matmul(Self, Int, Int) -> Int
fn ComputationGraph::mul(Self, Int, Int) -> Int
fn ComputationGraph::new() -> Self
fn ComputationGraph::relu(Self, Int) -> Int
fn ComputationGraph::sub(Self, Int, Int) -> Int
fn ComputationGraph::sum(Self, Int) -> Int
fn ComputationGraph::value_of(Self, Int) -> @math.Tensor

pub struct Dropout {
  rate : Double
  mut seed : Int
}
fn Dropout::forward(Self, @math.Matrix, train? : Bool) -> @math.Matrix
fn Dropout::new(Double, seed? : Int) -> Self

pub struct EarlyStopping {
  mode : EarlyStoppingMode
  patience : Int
  min_delta : Double
  mut best : Double
  mut num_bad : Int
  mut stopped : Bool
}
fn EarlyStopping::get_best(Self) -> Double
fn EarlyStopping::new(EarlyStoppingMode, Int, min_delta? : Double) -> Self
fn EarlyStopping::reset(Self) -> Unit
fn EarlyStopping::should_stop(Self) -> Bool
fn EarlyStopping::update(Self, Double) -> Bool

pub enum EarlyStoppingMode {
  Min
  Max
}

pub struct Embedding {
  weight : Array[Array[Double]]
  num_embeddings : Int
  dim : Int
  seed : Int
}
fn Embedding::forward(Self, Array[Int]) -> @math.Matrix
fn Embedding::new(Int, Int, seed? : Int) -> Self

pub struct EmbeddingContext {
  weight : AutoTensor
}

pub struct EmbeddingModule {
  weight : Parameter
  num_embeddings : Int
  dim : Int
}
fn EmbeddingModule::forward(Self, AutoTensor) -> (AutoTensor, EmbeddingContext)
fn EmbeddingModule::from_layer(Embedding) -> Self
fn EmbeddingModule::new(Int, Int, seed? : Int) -> Self
fn EmbeddingModule::parameters(Self) -> Array[Parameter]
fn EmbeddingModule::sync_grads(Self, EmbeddingContext) -> Unit

pub struct LayerNorm {
  gamma : Array[Double]
  beta : Array[Double]
  eps : Double
}
fn LayerNorm::forward(Self, @math.Matrix) -> @math.Matrix
fn LayerNorm::new(Int, eps? : Double) -> Self

pub struct LayerNormContext {
  gamma : AutoTensor
  beta : AutoTensor
}

pub struct LayerNormModule {
  gamma : Parameter
  beta : Parameter
  eps : Double
}
fn LayerNormModule::forward(Self, AutoTensor) -> (AutoTensor, LayerNormContext)
fn LayerNormModule::from_layer(LayerNorm) -> Self
fn LayerNormModule::new(Int, eps? : Double) -> Self
fn LayerNormModule::parameters(Self) -> Array[Parameter]
fn LayerNormModule::sync_grads(Self, LayerNormContext) -> Unit

pub struct Linear {
  mut w : Array[Array[Double]]
  mut b : Array[Double]
  in_features : Int
  out_features : Int
}
fn Linear::forward(Self, @math.Matrix) -> @math.Matrix
fn Linear::forward_tensor(Self, @math.Tensor) -> @math.Tensor
fn Linear::new(Int, Int) -> Self
fn Linear::set_params(Self, Array[Array[Double]], Array[Double]) -> Unit

pub struct LinearContext {
  weight : AutoTensor
  bias : AutoTensor
}

pub struct LinearModule {
  weight : Parameter
  bias : Parameter
  in_features : Int
  out_features : Int
}
fn LinearModule::forward(Self, AutoTensor) -> (AutoTensor, LinearContext)
fn LinearModule::new(Int, Int, seed? : Int) -> Self
fn LinearModule::parameters(Self) -> Array[Parameter]
fn LinearModule::set_params(Self, @math.Tensor, @math.Tensor) -> Unit
fn LinearModule::sync_grads(Self, LinearContext) -> Unit

pub struct MLP {
  layers : Array[Linear]
  activations : Array[Activation]
  output_activation : Activation
}
fn MLP::forward(Self, @math.Matrix) -> @math.Matrix
fn MLP::forward_tensor(Self, @math.Tensor) -> @math.Tensor
fn MLP::mlp_params(Self) -> Array[OptimParam]
fn MLP::mlp_predict(Self, @math.Matrix) -> Array[Int]
fn MLP::mlp_predict_proba(Self, @math.Matrix) -> @math.Matrix
fn MLP::mlp_predict_proba_tensor(Self, @math.Tensor) -> @math.Tensor
fn MLP::mlp_predict_tensor(Self, @math.Tensor) -> Array[Int]
fn MLP::mlp_set_params(Self, Array[OptimParam]) -> Unit
fn MLP::new(Int, Array[Int], Int, output_activation? : Activation, seed? : Int) -> Self

pub enum Module {
  Linear(LinearModule)
  Relu
  Dropout(Dropout)
  BatchNorm1d(BatchNorm1dModule)
  LayerNorm(LayerNormModule)
  Embedding(EmbeddingModule)
  Sequential(Sequential)
  ModuleList(ModuleList)
}

pub enum ModuleContext {
  None
  Linear(LinearContext)
  BatchNorm1d(BatchNorm1dContext)
  LayerNorm(LayerNormContext)
  Embedding(EmbeddingContext)
  Sequential(Array[ModuleContext])
}

pub struct ModuleList {
  modules : Array[Module]
  mut train : Bool
}
fn ModuleList::eval(Self) -> Unit
fn ModuleList::load_state_dict(Self, Map[String, @math.Tensor], strict? : Bool) -> Unit
fn ModuleList::new(Array[Module]) -> Self
fn ModuleList::parameters(Self) -> Array[Parameter]
fn ModuleList::state_dict(Self) -> Map[String, @math.Tensor]
fn ModuleList::train(Self) -> Unit

pub struct Node {
  value : @math.Tensor
  mut grad : @math.Tensor
  op : Op
  parents : Array[Int]
  requires_grad : Bool
  aux_tensors : Array[@math.Tensor]
  aux_scalars : Array[Double]
  aux_bool : Bool
}

pub enum Op {
  Input
  Add
  Sub
  Mul
  Relu
  MatMul
  AddBias
  Sum
  Dropout
  BatchNorm1d
  LayerNorm
  Embedding
}

pub struct OptimParam {
  data : Array[Double]
  grad : Array[Double]
}
fn OptimParam::new(Array[Double]) -> Self
fn OptimParam::with_grad(Self, Array[Double]) -> Self
fn OptimParam::zero_grad(Self) -> Unit

pub enum Optimizer {
  SGD(SGD)
  Adam(Adam)
  AdamW(AdamW)
  RMSProp(RMSProp)
  Adagrad(Adagrad)
}

pub struct Parameter {
  mut data : @math.Tensor
  mut grad : @math.Tensor
  requires_grad : Bool
}
fn Parameter::get_data(Self) -> @math.Tensor
fn Parameter::get_grad(Self) -> @math.Tensor
fn Parameter::new(@math.Tensor, requires_grad? : Bool) -> Self
fn Parameter::requires_grad(Self) -> Bool
fn Parameter::set_data(Self, @math.Tensor) -> Unit
fn Parameter::set_data_inplace(Self, @math.Tensor) -> Unit
fn Parameter::set_grad(Self, @math.Tensor) -> Unit
fn Parameter::zero_grad(Self) -> Unit

pub struct RMSProp {
  mut lr : Double
  alpha : Double
  eps : Double
  v : Array[Double]
}
fn RMSProp::get_lr(Self) -> Double
fn RMSProp::new(Double, Int, alpha? : Double, eps? : Double) -> Self
fn RMSProp::set_lr(Self, Double) -> Unit
fn RMSProp::step(Self, OptimParam) -> Unit

pub struct RMSPropParams {
  lr : Double
  alpha : Double
  eps : Double
  v : Array[Array[Double]]
}
fn RMSPropParams::new(Double, Array[Parameter], alpha? : Double, eps? : Double) -> Self
fn RMSPropParams::step(Self, Array[Parameter]) -> Unit
fn RMSPropParams::step_seq(Self, Sequential) -> Unit

pub struct SGD {
  mut lr : Double
  momentum : Double
  weight_decay : Double
  velocity : Array[Double]
}
fn SGD::get_lr(Self) -> Double
fn SGD::new(Double, Int, momentum? : Double, weight_decay? : Double) -> Self
fn SGD::set_lr(Self, Double) -> Unit
fn SGD::step(Self, OptimParam) -> Unit

pub struct SGDParams {
  lr : Double
  momentum : Double
  weight_decay : Double
  velocities : Array[Array[Double]]
}
fn SGDParams::new(Double, Array[Parameter], momentum? : Double, weight_decay? : Double) -> Self
fn SGDParams::step(Self, Array[Parameter]) -> Unit
fn SGDParams::step_seq(Self, Sequential) -> Unit

pub struct Sequential {
  modules : Array[Module]
  mut train : Bool
}
fn Sequential::eval(Self) -> Unit
fn Sequential::forward(Self, AutoTensor) -> (AutoTensor, Array[ModuleContext])
fn Sequential::load_state_dict(Self, Map[String, @math.Tensor], strict? : Bool) -> Unit
fn Sequential::new(Array[Module]) -> Self
fn Sequential::parameters(Self) -> Array[Parameter]
fn Sequential::state_dict(Self) -> Map[String, @math.Tensor]
fn Sequential::sync_grads(Self, Array[ModuleContext]) -> Unit
fn Sequential::train(Self) -> Unit

// Type aliases

// Traits

